{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d96c30b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import logging\n",
    "import json\n",
    "import argparse\n",
    "import os\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "51a0e288",
   "metadata": {},
   "outputs": [],
   "source": [
    "global DCAT, PROVID\n",
    "\n",
    "PROVID = None  #\"9ef60317-5da5-4050-8bbc-7d6826fee49f\"\n",
    "DCAT = None  #\"http://localhost:7000\"\n",
    "\n",
    "REGISTER_DATA = \"/datasets/register_datasets\"\n",
    "FIND_STDVARS = \"/knowledge_graph/find_standard_variables\"\n",
    "REGISTER_STDVARS = \"/knowledge_graph/register_standard_variables\"\n",
    "REGISTER_DSVARS = \"/datasets/register_variables\"\n",
    "REGISTER_RESOURCES = \"/datasets/register_resources\"\n",
    "RESOURCE_CHUNK_SIZE = 500\n",
    "SYNC_DSMETA = \"/datasets/sync_dataset_metadata\"\n",
    "PROVENANCE_URL = \"/provenance/register_provenance\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d59ce542",
   "metadata": {},
   "outputs": [],
   "source": [
    "def register_dataset(details: dict):\n",
    "    \"\"\"Create dataset, standard variables, dataset variables and resources\n",
    "\n",
    "    Args:\n",
    "        details (dict): Dataset file parsed as json\n",
    "    \"\"\"\n",
    "    dsid = None\n",
    "    # If a dataset id is present\n",
    "    # - Don't create a new dataset with new variables\n",
    "    if \"id\" in details:\n",
    "        # Get dataset id\n",
    "        dsid = details[\"id\"]\n",
    "    else:\n",
    "        # Register dataset\n",
    "        print(\"Registering dataset\")\n",
    "        dsid = create_dataset(details)\n",
    "\n",
    "        print(\"Registering Variables\")\n",
    "        # Register standard variables\n",
    "        if \"variables\" in details:\n",
    "            variables_details = get_json_from_path(details['variables'])\n",
    "            dsvars = create_standard_variables(variables_details)\n",
    "            create_dataset_variables(dsid, dsvars)\n",
    "\n",
    "    print(\"Registering Resources\")\n",
    "    # Register resources\n",
    "    resources = get_resources_json(details[\"resources\"])\n",
    "    if resources is not None and len(resources) > 0:\n",
    "        create_resources(dsid, resources)\n",
    "    return dsid\n",
    "\n",
    "\n",
    "def sync_dataset_metadata(dsid):\n",
    "    payload = {\n",
    "        \"dataset_id\": dsid\n",
    "    }\n",
    "    submit_request(SYNC_DSMETA, payload, True)\n",
    "\n",
    "\n",
    "def create_dataset(details):\n",
    "    \"\"\"Create dataset\n",
    "\n",
    "    Args:\n",
    "        details (dict): Dataset file parsed as json\n",
    "\n",
    "    Returns:\n",
    "        str: dataset id\n",
    "    \"\"\"\n",
    "    # TODO: check for existing dataset with same name ?\n",
    "    payload = {\n",
    "        \"datasets\": [{\n",
    "            \"provenance_id\": PROVID,\n",
    "            \"name\": details[\"name\"],\n",
    "            \"description\": details[\"description\"],\n",
    "            \"metadata\": details[\"metadata\"]\n",
    "        }]\n",
    "    }\n",
    "    result = submit_request(REGISTER_DATA, payload)\n",
    "    if result is None or len(result[\"datasets\"]) == 0:\n",
    "        return None\n",
    "\n",
    "    dsid = result[\"datasets\"][0][\"record_id\"]\n",
    "    return dsid\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "93f30185",
   "metadata": {},
   "outputs": [],
   "source": [
    "# - Find Standard Variables if they exist\n",
    "# - Otherwise create Standard Variables for the ones that don't\n",
    "#   - Get standard variable ids\n",
    "def create_standard_variables(variables_details: dict) -> dict:\n",
    "    \"\"\"Create standard variables if they don't exist\n",
    "\n",
    "    Args:\n",
    "        variables_details (dict): the details of the variables from the file\n",
    "\n",
    "    Returns:\n",
    "        dict: the standard variables created\n",
    "    \"\"\"\n",
    "    if variables_details is not None and len(variables_details) > 0:\n",
    "        stdvars = []\n",
    "        for dsvar in variables_details:\n",
    "            stdvars.extend(dsvar['standard_variables'])\n",
    "\n",
    "        find_existing_json = {\n",
    "            \"name__in\": list(map(lambda var: var[\"name\"], stdvars))\n",
    "        }\n",
    "        find_result = submit_request(FIND_STDVARS, find_existing_json)\n",
    "\n",
    "        cur_stdvars = {}\n",
    "        if find_result is not None and len(\n",
    "                find_result[\"standard_variables\"]) > 0:\n",
    "            for stdvar in find_result[\"standard_variables\"]:\n",
    "                cur_stdvars[stdvar[\"name\"]] = stdvar\n",
    "\n",
    "        new_stdvars = []\n",
    "        for stdvar in stdvars:\n",
    "            if stdvar[\"name\"] not in cur_stdvars:\n",
    "                new_stdvars.append(stdvar)\n",
    "\n",
    "        if len(new_stdvars) > 0:\n",
    "            register_json = {\"standard_variables\": new_stdvars}\n",
    "            register_result = submit_request(REGISTER_STDVARS, register_json)\n",
    "            if register_result is not None and len(\n",
    "                    register_result[\"standard_variables\"]) > 0:\n",
    "                for stdvar in register_result[\"standard_variables\"]:\n",
    "                    stdvar[\"id\"] = stdvar[\"record_id\"]\n",
    "                    cur_stdvars[stdvar[\"name\"]] = stdvar\n",
    "\n",
    "        new_dsvars = []\n",
    "        for dsvar in variables_details:\n",
    "            new_dsvar = {\n",
    "                \"name\": dsvar[\"name\"],\n",
    "                \"metadata\": dsvar[\"metadata\"],\n",
    "                \"standard_variable_ids\": []\n",
    "            }\n",
    "            for stdvar in dsvar[\"standard_variables\"]:\n",
    "                stdvarname = stdvar[\"name\"]\n",
    "                if stdvarname in cur_stdvars:\n",
    "                    new_dsvar[\"standard_variable_ids\"].append(\n",
    "                        cur_stdvars[stdvarname]['id'])\n",
    "\n",
    "            new_dsvars.append(new_dsvar)\n",
    "\n",
    "        return new_dsvars\n",
    "\n",
    "\n",
    "# - Create dataset variables\n",
    "#   - name, [standard_variable_ids], dataset_id\n",
    "def create_dataset_variables(dataset_id: str, dataset_variables: dict) -> dict:\n",
    "    \"\"\"Create the dataset variables\n",
    "\n",
    "    Args:\n",
    "        dataset_id (str): dataset id\n",
    "        dataset_variables (dict): dataset variables\n",
    "\n",
    "    Returns:\n",
    "        dict: the dataset variables created\n",
    "    \"\"\"\n",
    "    if dataset_id is not None and dataset_variables is not None and len(\n",
    "            dataset_variables) > 0:\n",
    "        for dsvar in dataset_variables:\n",
    "            dsvar[\"dataset_id\"] = dataset_id\n",
    "        request = {\"variables\": dataset_variables}\n",
    "        response = submit_request(REGISTER_DSVARS, request)\n",
    "        if response is not None:\n",
    "            return response[\"variables\"]\n",
    "        else:\n",
    "            return {}\n",
    "\n",
    "\n",
    "# Add resources to dataset\n",
    "# - Add provenance and dataset id to the resources\n",
    "# - Register resources upto RESOURCE_CHUNK_SIZE in one go\n",
    "def create_resources(dataset_id, resources):\n",
    "    \"\"\"Create the resource for the dataset\n",
    "\n",
    "    Args:\n",
    "        dataset_id (str): the dataset id\n",
    "        resources (dict): the resources to be added to the dataset\n",
    "    \"\"\"\n",
    "    if dataset_id is not None and resources is not None and len(resources) > 0:\n",
    "        resource_chunks = divide_chunks(resources, RESOURCE_CHUNK_SIZE)\n",
    "        chunkid = 1\n",
    "        for chunk in resource_chunks:\n",
    "            print(f\"Registering resource chunk {chunkid}\")\n",
    "            chunkid += 1\n",
    "            chunklist = list(chunk)\n",
    "            for resource in chunklist:\n",
    "                resource[\"provenance_id\"] = PROVID\n",
    "                resource[\"dataset_id\"] = dataset_id\n",
    "            register_json = {\"resources\": chunklist}\n",
    "            submit_request(REGISTER_RESOURCES, register_json)\n",
    "\n",
    "\n",
    "# Fetch resources from json\n",
    "# - If resources is a string, read from file, otherwise read resources list\n",
    "def get_resources_json(resources):\n",
    "    \"\"\"Get the resources from the json\"\"\"\n",
    "    if isinstance(resources, list):\n",
    "        return resources\n",
    "    if isinstance(resources, str):\n",
    "        with open(resources, \"r\") as infile:\n",
    "            return json.load(infile)\n",
    "\n",
    "\n",
    "# Fetch variables from json\n",
    "# - If variables is a string, read from file, otherwise read variables list\n",
    "def get_json_from_path(path):\n",
    "    \"\"\"Get json file from path\n",
    "\n",
    "    Args:\n",
    "        path (str): the path of the file\n",
    "\n",
    "    Returns:\n",
    "        dict: payload\n",
    "    \"\"\"\n",
    "    with open(path, \"r\") as infile:\n",
    "        return json.load(infile)\n",
    "\n",
    "\n",
    "# Helper function to submit a request to the data catalog\n",
    "def submit_request(url: str, payload: dict, skipReturn=False) -> dict:\n",
    "    \"\"\"Send a HTTP request to the datacatalog server\n",
    "\n",
    "    Args:\n",
    "        url (str): the url of the datacatalog server\n",
    "        payload (dict): the json to send to the server\n",
    "\n",
    "    Returns:\n",
    "        dict: the response from the server \n",
    "    \"\"\"\n",
    "    try:\n",
    "        r = requests.post(DCAT + url, json=payload)\n",
    "        r.raise_for_status()\n",
    "    except requests.exceptions.HTTPError:\n",
    "        logging.error(r.json())\n",
    "        logging.error(\"Error request\", exc_info=True)\n",
    "        exit(1)\n",
    "    if r.status_code == 200:\n",
    "        if not skipReturn:\n",
    "            result = r.json()\n",
    "            if result[\"result\"] == \"success\":\n",
    "                print(result)\n",
    "                return result\n",
    "            else:\n",
    "                logging.error(f\"Response is not success {result}\")\n",
    "    else:\n",
    "        logging.error(\"Error request\")\n",
    "    return None\n",
    "\n",
    "\n",
    "# Yield successive n-sized chunks from l.\n",
    "def divide_chunks(l, n):\n",
    "    # looping till length l\n",
    "    for i in range(0, len(l), n):\n",
    "        yield l[i:i + n]\n",
    "\n",
    "\n",
    "def create_provenance_id(provenance_id):\n",
    "    provenance_definition = {\n",
    "        \"provenance\": {\n",
    "            \"provenance_type\": \"user\",\n",
    "            \"record_id\": provenance_id,\n",
    "            \"name\": \"test_api_outside\",\n",
    "            \"metadata\": {\n",
    "                \"contact_information\": {\n",
    "                    \"email\": \"email@example.com\"\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "    try:\n",
    "        r = requests.post(f\"\"\"{DCAT}{PROVENANCE_URL}\"\"\",\n",
    "                          json=provenance_definition)\n",
    "        r.raise_for_status()\n",
    "    except requests.exceptions.HTTPError as err:\n",
    "        logging.error(r.json())\n",
    "        logging.error(\"Error request\", exc_info=True)\n",
    "        exit(1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8f3ff454",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def main():\n",
    "#     \"\"\"Main function\n",
    "#     \"\"\"\n",
    "#     global DCAT, PROVID\n",
    "\n",
    "#     parser = argparse.ArgumentParser(description='Register a dataset')\n",
    "#     parser.add_argument('FILE', help='dataset details file',default=\"C:/Users/khush/OneDrive/Desktop/example_mint/dataset.json\")\n",
    "#     parser.add_argument('DATA_CATALOG_URL',\n",
    "#                         help='data catalog url',\n",
    "#                         default=\"https://data-catalog.dev.mint.isi.edu\")\n",
    "#     parser.add_argument('PROVENANCE_ID',\n",
    "#                         help='provenance id of the user',\n",
    "#                         default=\"9ef60317-5da5-4050-8bbc-7d6826fee49f\")\n",
    "\n",
    "#     args = parser.parse_args(args=[])\n",
    "#     DCAT = args.DATA_CATALOG_URL\n",
    "#     PROVID = args.PROVENANCE_ID\n",
    "\n",
    "#     create_provenance_id(PROVID)\n",
    "#     with open(args.FILE, \"r\") as details_file:\n",
    "#         dir_name = os.path.dirname(os.path.join(os.getcwd(), args.FILE))\n",
    "#         os.chdir(dir_name)\n",
    "#         details = json.load(details_file)\n",
    "#         dsid = register_dataset(details)\n",
    "#         sync_dataset_metadata(dsid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "bb0ec51f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Registering dataset\n",
      "{'result': 'success', 'datasets': [{'record_id': '9d93f00a-d4de-47f7-9d23-ad327a216027', 'provenance_id': '9ef60317-5da5-4050-8bbc-7d6826fee49f', 'name': 'EDWARDS_BFZ-BARTON_SPRINGS-TRANSIENT-1989_1998_v4', 'description': 'Groundwater flow model developed for the Barton Springs segment of the Edwards Aquifer developed by the Bureau of Economic Geology, TWDB and Barton Springs Edwards Aquifer Conservation District. This region is hydrologically distinct from other parts of the Edwards Aquifer and is a major source of water.', 'json_metadata': {'temporal_coverage': {'start_time': '1989-01-01T00:00:00', 'end_time': '1998-01-01T00:00:00'}, 'datatype': 'modflow'}}]}\n",
      "Registering Variables\n",
      "{'result': 'success', 'standard_variables': [{'id': '65a6e85a-26d0-5a1f-bc37-3b0744f8adf4', 'ontology': 'SVO', 'name': 'aquifer_elevation', 'uri': 'http://www.geoscienceontology.org/svo/svl/variable#%28ground%40medium_water%29%40role%7Emain_water%40role%7aquifer_elevation', 'description': ''}, {'id': '97f89b28-0dd7-5a0f-9b16-1c5b779bd93a', 'ontology': 'SVO', 'name': 'aquifer_specific_yield', 'uri': 'http://www.geoscienceontology.org/svo/svl/variable#%28ground%40medium_water%29%40role%7Emain_water%40role%7aquifer_specific_yield', 'description': ''}, {'id': '9f017e2f-92e1-5fb9-9002-3aee59f3ec83', 'ontology': 'SVO', 'name': 'aquifer_horizontal_hydraulic_conductivity', 'uri': 'http://www.geoscienceontology.org/svo/svl/variable#%28ground%40medium_water%29%40role%7Emain_water%40role%7aquifer_horizontal_hydraulic_conductivity', 'description': ''}, {'id': 'f1ef8bb0-a32c-576e-822d-b3e337580118', 'ontology': 'SVO', 'name': 'aquifer_specific_storage', 'uri': 'http://www.geoscienceontology.org/svo/svl/variable#%28ground%40medium_water%29%40role%7Emain_water%40role%7Ein_recharge__recharge_volume_flux', 'description': ''}]}\n",
      "{'result': 'success', 'variables': [{'record_id': '0ff62ed8-9b43-4108-a46f-e4c05dd8727c', 'dataset_id': '9d93f00a-d4de-47f7-9d23-ad327a216027', 'name': 'elevation', 'json_metadata': {'label': 'elevation', 'units': 'ft', 'data_type': 'float', 'type': 'numerical.continuous'}}, {'record_id': '2dd6db24-dc0f-4867-911e-1b047fe9e641', 'dataset_id': '9d93f00a-d4de-47f7-9d23-ad327a216027', 'name': 'Horizontal hydraulic conductivity', 'json_metadata': {'label': 'Horizontal hydraulic conductivity', 'units': 'ft/d', 'data_type': 'float', 'type': 'numerical.continuous'}}, {'record_id': 'd05b0170-115f-402b-9c32-dd6c6d4c88aa', 'dataset_id': '9d93f00a-d4de-47f7-9d23-ad327a216027', 'name': 'specific yield', 'json_metadata': {'label': 'specific yield', 'units': '', 'data_type': 'float', 'type': 'numerical.continuous'}}, {'record_id': 'c2558739-c7ce-4419-a8ba-d668647b6fc1', 'dataset_id': '9d93f00a-d4de-47f7-9d23-ad327a216027', 'name': 'specific storage', 'json_metadata': {'label': 'specific storage', 'units': 'ft-1', 'data_type': 'float', 'type': 'numerical.continuous'}}]}\n",
      "Registering Resources\n",
      "Registering resource chunk 1\n",
      "{'result': 'success', 'resources': [{'record_id': 'b6193fe7-3a1d-423f-9eb9-4ca3d6478a72', 'provenance_id': '9ef60317-5da5-4050-8bbc-7d6826fee49f', 'dataset_id': '9d93f00a-d4de-47f7-9d23-ad327a216027', 'name': 'EDWARDS_BFZ-BARTON_SPRINGS-TRANSIENT-1989_1998', 'resource_type': 'model', 'data_url': 'https://tacc.mint.isi.edu/texas/models/configure/MODFLOW/modflow_2005/modflow_2005_cfg/75e7e090-de07-42f4-a652-8202a8b1bd2e', 'layout': {}, 'json_metadata': {'spatial_coverage': {'type': 'BoundingBox', 'value': {'xmax': -99, 'xmin': -100, 'ymax': 33, 'ymin': 34}}, 'temporal_coverage': {'start_time': '1989-01-01T00:00:00', 'end_time': '1998-01-01T00:00:00'}}}, {'record_id': '75cf846b-7a04-45f0-ac7e-df320b8071e6', 'provenance_id': '9ef60317-5da5-4050-8bbc-7d6826fee49f', 'dataset_id': '9d93f00a-d4de-47f7-9d23-ad327a216027', 'name': 'EDWARDS_BFZ-BARTON_SPRINGS-TRANSIENT-1989_1998.zip', 'resource_type': 'Input data zip file', 'data_url': 'https://portals-api.tacc.utexas.edu/postits/v2/38e48b08-b6bc-44e9-b105-f8063289d890-010', 'layout': {}, 'json_metadata': {'spatial_coverage': {'type': 'BoundingBox', 'value': {'xmax': -99, 'xmin': -100, 'ymax': 33, 'ymin': 34}}, 'temporal_coverage': {'start_time': '1989-01-01T00:00:00', 'end_time': '1998-01-01T00:00:00'}}}]}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "    \"\"\"Main function\n",
    "    \"\"\"\n",
    "    global DCAT, PROVID\n",
    "\n",
    "#     parser = argparse.ArgumentParser(description='Register a dataset')\n",
    "#     parser.add_argument('FILE', help='dataset details file',default=\"C:/Users/khush/OneDrive/Desktop/example_mint/dataset.json\")\n",
    "#     parser.add_argument('DATA_CATALOG_URL',\n",
    "#                         help='data catalog url',\n",
    "#                         default=\"https://data-catalog.dev.mint.isi.edu\")\n",
    "#     parser.add_argument('PROVENANCE_ID',\n",
    "#                         help='provenance id of the user',\n",
    "#                         default=\"9ef60317-5da5-4050-8bbc-7d6826fee49f\")\n",
    "\n",
    "#     args = parser.parse_args(args=[])\n",
    "    #DCAT = \"https://data-catalog.dev.mint.isi.edu\"\n",
    "    DCAT = \"https://data-catalog.tacc.mint.isi.edu\"\n",
    "    PROVID = \"9ef60317-5da5-4050-8bbc-7d6826fee49f\"\n",
    "    #FILE = \"C:/Users/khush/OneDrive/Desktop/example_mint/dataset.json\"\n",
    "    FILE = \"C:/Users/khush/OneDrive/Desktop/example_mint/dataset_Barton.json\"\n",
    "    create_provenance_id(PROVID)\n",
    "    with open(FILE, \"r\") as details_file:\n",
    "        dir_name = os.path.dirname(os.path.join(os.getcwd(),FILE))\n",
    "        os.chdir(dir_name)\n",
    "        details = json.load(details_file)\n",
    "        dsid = register_dataset(details)\n",
    "        sync_dataset_metadata(dsid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "26169851",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5a73105c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "86272737",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ac7dd7a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
